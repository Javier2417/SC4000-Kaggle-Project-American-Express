{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111d2729",
   "metadata": {},
   "source": [
    "# Amex 1st Place Pipeline - Step 1: Feature Engineering\n",
    "\n",
    "This notebook replicates the core feature engineering from the 1st place solution (`S1_denoise.py` and `S2_manual_feature.py`). \n",
    "\n",
    "**Pipeline:**\n",
    "1.  **Load Raw Data**: Load `train.parquet` and `test.parquet` (denoised versions from `S1`).\n",
    "2.  **Define Feature Types**: Identify `CAT_FEATURES` and `NUM_FEATURES`.\n",
    "3.  **Create `lastk` Features**: Generate aggregate features for the **last 3** and **last 6** statements.\n",
    "4.  **Create `rank` Features**: Generate within-customer percentile rank features.\n",
    "5.  **Create `diff` Features**: Generate difference-from-lag features.\n",
    "6.  **Create Main Aggregates**: Generate aggregates over the *entire* history.\n",
    "7.  **Combine & Save**: Merge all feature sets and save to `train_fe.parquet` and `test_fe.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846ecb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04324404",
   "metadata": {},
   "source": [
    "## Step 1: Define Paths & Feature Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03992d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths ---\n",
    "# IMPORTANT: This notebook assumes you have already run S1_denoise.py\n",
    "# or have the 'denoised' parquet files available.\n",
    "# For this example, we will use the 'optimized' files from our v3 notebook\n",
    "# and apply denoising manually.\n",
    "\n",
    "PARQUET_DATA_DIR = '../data_parquet/'\n",
    "CSV_DATA_DIR = '../data/'\n",
    "FE_OUTPUT_DIR = '../data_fe/' # New directory to save our features\n",
    "\n",
    "if not os.path.exists(FE_OUTPUT_DIR):\n",
    "    os.makedirs(FE_OUTPUT_DIR)\n",
    "\n",
    "TRAIN_IN_PATH = os.path.join(PARQUET_DATA_DIR, 'train_optimized.parquet')\n",
    "TEST_IN_PATH = os.path.join(PARQUET_DATA_DIR, 'test_optimized.parquet') \n",
    "LABELS_PATH = os.path.join(CSV_DATA_DIR, 'train_labels.csv')\n",
    "\n",
    "TRAIN_OUT_PATH = os.path.join(FE_OUTPUT_DIR, 'train_fe.parquet')\n",
    "TEST_OUT_PATH = os.path.join(FE_OUTPUT_DIR, 'test_fe.parquet')\n",
    "\n",
    "\n",
    "# --- Features ---\n",
    "CAT_FEATURES = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "# We will define NUM_FEATURES after loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370540ad",
   "metadata": {},
   "source": [
    "## Step 2: Denoising & Pre-Feature Engineering (S1 & S2 logic)\n",
    "\n",
    "This function applies the 1st place solution's key ideas *before* aggregation:\n",
    "1.  **Denoising**: Floors float values (`*100`).\n",
    "2.  **Categorical Mapping**: Converts `D_63`/`D_64` to integers.\n",
    "3.  **Lag/Diff Features**: Calculates `diff(1)`.\n",
    "4.  **Rank Features**: Calculates `rank(pct=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313e9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_and_pre_feature_engineer(df, num_features):\n",
    "    print(\"Starting denoising and pre-feature engineering...\")\n",
    "    \n",
    "    # 1. Denoise (from S1_denoise.py)\n",
    "    print(\"Applying 'denoising' (floor * 100)...\")\n",
    "    for col in tqdm(num_features):\n",
    "        df[col] = np.floor(df[col] * 100)\n",
    "    \n",
    "    # 2. Categorical Mapping (from S1_denoise.py)\n",
    "    print(\"Applying manual categorical mapping...\")\n",
    "    df['D_63'] = df['D_63'].apply(lambda t: {'CR':0, 'XZ':1, 'XM':2, 'CO':3, 'CL':4, 'XL':5, np.nan:-1}.get(t, -1)).astype(np.int8)\n",
    "    df['D_64'] = df['D_64'].apply(lambda t: {np.nan:-1, 'O':0, '-1':1, 'R':2, 'U':3}.get(t, -1)).astype(np.int8)\n",
    "    \n",
    "    # 3. Lag/Diff Features (from S2_manual_feature.py)\n",
    "    print(\"Creating lag/diff features...\")\n",
    "    cid_map = df['customer_ID'].to_dict()\n",
    "    df_diff = df.groupby('customer_ID')[num_features].diff(1).add_prefix('diff_')\n",
    "    df_diff['customer_ID'] = cid_map # Add customer_ID back for merging later\n",
    "    \n",
    "    # 4. Rank Features (from S2_manual_feature.py)\n",
    "    print(\"Creating within-customer rank features...\")\n",
    "    df_rank = df.groupby('customer_ID')[num_features].rank(pct=True).add_prefix('rank_')\n",
    "    df_rank['customer_ID'] = cid_map # Add customer_ID back\n",
    "    \n",
    "    print(\"Pre-feature engineering complete.\")\n",
    "    return df, df_diff, df_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2_agg_markdown",
   "metadata": {},
   "source": [
    "## Step 3: Aggregation Function (S2 Logic)\n",
    "\n",
    "This function aggregates the pre-engineered features. It's designed to be called for the full dataset, the last 3 statements, and the last 6 statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "s2_agg_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_aggregate(df, df_diff, df_rank, num_features, prefix=''):\n",
    "    print(f\"Starting aggregation with prefix: '{prefix}'...\")\n",
    "    \n",
    "    # Define features for this aggregation\n",
    "    diff_features = [f'diff_{col}' for col in num_features]\n",
    "    rank_features = [f'rank_{col}' for col in num_features]\n",
    "\n",
    "    # Group by customer_ID\n",
    "    df_grouped = df.groupby(\"customer_ID\")\n",
    "    df_diff_grouped = df_diff.groupby(\"customer_ID\")\n",
    "    df_rank_grouped = df_rank.groupby(\"customer_ID\")\n",
    "\n",
    "    # 1. Numerical Aggregates (mean, std, min, max, sum, last)\n",
    "    print(\"Aggregating numerical features...\")\n",
    "    num_agg = df_grouped[num_features].agg(['mean', 'std', 'min', 'max', 'sum', 'last'])\n",
    "    num_agg.columns = ['_'.join(x) for x in num_agg.columns]\n",
    "    \n",
    "    # 2. Diff Aggregates (mean, std, min, max, sum, last)\n",
    "    print(\"Aggregating diff features...\")\n",
    "    diff_agg = df_diff_grouped[diff_features].agg(['mean', 'std', 'min', 'max', 'sum', 'last'])\n",
    "    diff_agg.columns = ['_'.join(x) for x in diff_agg.columns]\n",
    "    \n",
    "    # 3. Rank Aggregates (last)\n",
    "    print(\"Aggregating rank features...\")\n",
    "    rank_agg = df_rank_grouped[rank_features].agg(['last'])\n",
    "    rank_agg.columns = ['_'.join(x) for x in rank_agg.columns]\n",
    "    \n",
    "    # 4. Categorical Aggregates (last, nunique, count)\n",
    "    print(\"Aggregating categorical features...\")\n",
    "    cat_agg = df_grouped[CAT_FEATURES].agg(['last', 'nunique', 'count'])\n",
    "    cat_agg.columns = ['_'.join(x) for x in cat_agg.columns]\n",
    "    \n",
    "    # 5. Statement Count\n",
    "    count_agg = df_grouped[['S_2']].agg(['count'])\n",
    "    count_agg.columns = ['statement_count']\n",
    "\n",
    "    # Combine all aggregates\n",
    "    print(\"Combining aggregates...\")\n",
    "    df_agg = pd.concat([num_agg, diff_agg, rank_agg, cat_agg, count_agg], axis=1).reset_index()\n",
    "    \n",
    "    # Add prefix (e.g., 'last3_') to all columns except customer_ID\n",
    "    if prefix != '':\n",
    "        df_agg = df_agg.add_prefix(prefix)\n",
    "        df_agg = df_agg.rename(columns={f'{prefix}customer_ID': 'customer_ID'}) \n",
    "    \n",
    "    print(f\"Aggregation for '{prefix}' complete. Shape: {df_agg.shape}\")\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbc1cb",
   "metadata": {},
   "source": [
    "## Step 4: Process Full Training Data\n",
    "\n",
    "This cell executes the full S1 -> S2 pipeline for the **training data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e895887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full training data from ../data_parquet/train_optimized.parquet...\n",
      "Train data loaded in 4.38s. Shape: (5531451, 190)\n",
      "Starting denoising and pre-feature engineering...\n",
      "Applying 'denoising' (floor * 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:03<00:00, 57.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying manual categorical mapping...\n",
      "Creating lag/diff features...\n",
      "Creating within-customer rank features...\n",
      "Pre-feature engineering complete.\n",
      "--- Starting All Aggregations for TRAIN ---\n",
      "Starting aggregation with prefix: ''...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for '' complete. Shape: (458913, 2336)\n",
      "Starting aggregation with prefix: 'last3_'...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for 'last3_' complete. Shape: (458913, 2336)\n",
      "Starting aggregation with prefix: 'last6_'...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for 'last6_' complete. Shape: (458913, 2336)\n",
      "Combining all TRAIN feature sets...\n",
      "Loading and merging labels...\n",
      "Full training set ready. Shape: (458913, 7007)\n",
      "Saving processed train features to ../data_fe/train_fe.parquet...\n",
      "Total time for train processing: 623.81s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading full training data from {TRAIN_IN_PATH}...\")\n",
    "start_time = time.time()\n",
    "train_df = pd.read_parquet(TRAIN_IN_PATH)\n",
    "print(f\"Train data loaded in {time.time() - start_time:.2f}s. Shape: {train_df.shape}\")\n",
    "\n",
    "# Define NUM_FEATURES now that we have the df\n",
    "all_cols = [c for c in list(train_df.columns) if c not in ['customer_ID', 'S_2']]\n",
    "NUM_FEATURES = [col for col in all_cols if col not in CAT_FEATURES]\n",
    "\n",
    "# Apply S1 Denoising & S2 Pre-FE\n",
    "train_denoised, train_diff, train_rank = denoise_and_pre_feature_engineer(train_df, NUM_FEATURES)\n",
    "del train_df; gc.collect()\n",
    "\n",
    "# --- S2 Aggregations ---\n",
    "print(\"--- Starting All Aggregations for TRAIN ---\")\n",
    "\n",
    "# 1. Aggregate ALL statements (prefix='')\n",
    "train_agg_all = process_and_aggregate(train_denoised, train_diff, train_rank, NUM_FEATURES, prefix='')\n",
    "\n",
    "# 2. Aggregate LAST 3 statements (prefix='last3_')\n",
    "train_denoised['rank'] = train_denoised.groupby('customer_ID')['S_2'].rank(ascending=False)\n",
    "train_last3 = train_denoised.loc[train_denoised['rank'] <= 3].reset_index(drop=True)\n",
    "train_agg_last3 = process_and_aggregate(train_last3, train_diff, train_rank, NUM_FEATURES, prefix='last3_')\n",
    "del train_last3; gc.collect()\n",
    "\n",
    "# 3. Aggregate LAST 6 statements (prefix='last6_')\n",
    "train_last6 = train_denoised.loc[train_denoised['rank'] <= 6].reset_index(drop=True)\n",
    "train_agg_last6 = process_and_aggregate(train_last6, train_diff, train_rank, NUM_FEATURES, prefix='last6_')\n",
    "del train_last6, train_denoised, train_diff, train_rank; gc.collect()\n",
    "\n",
    "# --- Combine all feature sets ---\n",
    "print(\"Combining all TRAIN feature sets...\")\n",
    "X_train = train_agg_all.merge(train_agg_last3, how='inner', on='customer_ID')\n",
    "X_train = X_train.merge(train_agg_last6, how='inner', on='customer_ID')\n",
    "\n",
    "print(\"Loading and merging labels...\")\n",
    "train_labels = pd.read_csv(LABELS_PATH)\n",
    "X_train = X_train.merge(train_labels, on='customer_ID', how='left')\n",
    "\n",
    "print(f\"Full training set ready. Shape: {X_train.shape}\")\n",
    "\n",
    "print(f\"Saving processed train features to {TRAIN_OUT_PATH}...\")\n",
    "X_train.to_parquet(TRAIN_OUT_PATH, index=False)\n",
    "\n",
    "del X_train, train_labels, train_agg_all, train_agg_last3, train_agg_last6\n",
    "gc.collect()\n",
    "print(f\"Total time for train processing: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c247c8",
   "metadata": {},
   "source": [
    "## Step 5: Process Test Data\n",
    "\n",
    "This cell executes the full S1 -> S2 pipeline for the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e098be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full test data from ../data_parquet/test_optimized.parquet...\n",
      "Test data loaded in 9.21s. Shape: (11363762, 190)\n",
      "Starting denoising and pre-feature engineering...\n",
      "Applying 'denoising' (floor * 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [00:06<00:00, 27.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying manual categorical mapping...\n",
      "Creating lag/diff features...\n",
      "Creating within-customer rank features...\n",
      "Pre-feature engineering complete.\n",
      "--- Starting All Aggregations for TEST ---\n",
      "Starting aggregation with prefix: ''...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for '' complete. Shape: (924621, 2336)\n",
      "Starting aggregation with prefix: 'last3_'...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for 'last3_' complete. Shape: (924621, 2336)\n",
      "Starting aggregation with prefix: 'last6_'...\n",
      "Aggregating numerical features...\n",
      "Aggregating diff features...\n",
      "Aggregating rank features...\n",
      "Aggregating categorical features...\n",
      "Combining aggregates...\n",
      "Aggregation for 'last6_' complete. Shape: (924621, 2336)\n",
      "Combining all TEST feature sets...\n",
      "Full test set ready. Shape: (924621, 7006)\n",
      "Saving processed test features to ../data_fe/test_fe.parquet...\n",
      "Total time for test processing: 1883.84s\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading full test data from {TEST_IN_PATH}...\")\n",
    "start_time = time.time()\n",
    "test_df = pd.read_parquet(TEST_IN_PATH)\n",
    "print(f\"Test data loaded in {time.time() - start_time:.2f}s. Shape: {test_df.shape}\")\n",
    "\n",
    "# Apply S1 Denoising & S2 Pre-FE\n",
    "# Note: We use the *same* NUM_FEATURES list defined from the train set\n",
    "test_denoised, test_diff, test_rank = denoise_and_pre_feature_engineer(test_df, NUM_FEATURES)\n",
    "del test_df; gc.collect()\n",
    "\n",
    "# --- S2 Aggregations ---\n",
    "print(\"--- Starting All Aggregations for TEST ---\")\n",
    "\n",
    "# 1. Aggregate ALL statements (prefix='')\n",
    "test_agg_all = process_and_aggregate(test_denoised, test_diff, test_rank, NUM_FEATURES, prefix='')\n",
    "\n",
    "# 2. Aggregate LAST 3 statements (prefix='last3_')\n",
    "test_denoised['rank'] = test_denoised.groupby('customer_ID')['S_2'].rank(ascending=False)\n",
    "test_last3 = test_denoised.loc[test_denoised['rank'] <= 3].reset_index(drop=True)\n",
    "test_agg_last3 = process_and_aggregate(test_last3, test_diff, test_rank, NUM_FEATURES, prefix='last3_')\n",
    "del test_last3; gc.collect()\n",
    "\n",
    "# 3. Aggregate LAST 6 statements (prefix='last6_')\n",
    "test_last6 = test_denoised.loc[test_denoised['rank'] <= 6].reset_index(drop=True)\n",
    "test_agg_last6 = process_and_aggregate(test_last6, test_diff, test_rank, NUM_FEATURES, prefix='last6_')\n",
    "del test_last6, test_denoised, test_diff, test_rank; gc.collect()\n",
    "\n",
    "# --- Combine all feature sets ---\n",
    "print(\"Combining all TEST feature sets...\")\n",
    "X_test = test_agg_all.merge(test_agg_last3, how='inner', on='customer_ID')\n",
    "X_test = X_test.merge(test_agg_last6, how='inner', on='customer_ID')\n",
    "\n",
    "print(f\"Full test set ready. Shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"Saving processed test features to {TEST_OUT_PATH}...\")\n",
    "X_test.to_parquet(TEST_OUT_PATH, index=False)\n",
    "\n",
    "del X_test, test_agg_all, test_agg_last3, test_agg_last6\n",
    "gc.collect()\n",
    "print(f\"Total time for test processing: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe_done_markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Complete\n",
    "\n",
    "This notebook has created and saved `train_fe.parquet` and `test_fe.parquet` in the `../data_fe/` directory.\n",
    "\n",
    "You can now proceed to the **`v4_LGBM_Training.ipynb`** notebook to load these powerful features and train the final model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
