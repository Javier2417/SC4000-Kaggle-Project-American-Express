{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76229c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting ../data/train_data.csv to parquet...\n",
      "  Processing train chunk 1...\n",
      "  Processing train chunk 2...\n",
      "  Processing train chunk 3...\n",
      "  Processing train chunk 4...\n",
      "  Processing train chunk 5...\n",
      "  Processing train chunk 6...\n",
      "  Processing train chunk 7...\n",
      "  Processing train chunk 8...\n",
      "  Processing train chunk 9...\n",
      "  Processing train chunk 10...\n",
      "  Processing train chunk 11...\n",
      "  Processing train chunk 12...\n",
      "Concatenating train chunks...\n",
      "Saving to ../data_parquet/train.parquet...\n",
      "Train data conversion complete.\n",
      "\n",
      "Converting ../data/test_data.csv to parquet...\n",
      "  Processing test chunk 1...\n",
      "  Processing test chunk 2...\n",
      "  Processing test chunk 3...\n",
      "  Processing test chunk 4...\n",
      "  Processing test chunk 5...\n",
      "  Processing test chunk 6...\n",
      "  Processing test chunk 7...\n",
      "  Processing test chunk 8...\n",
      "  Processing test chunk 9...\n",
      "  Processing test chunk 10...\n",
      "  Processing test chunk 11...\n",
      "  Processing test chunk 12...\n",
      "  Processing test chunk 13...\n",
      "  Processing test chunk 14...\n",
      "  Processing test chunk 15...\n",
      "  Processing test chunk 16...\n",
      "  Processing test chunk 17...\n",
      "  Processing test chunk 18...\n",
      "  Processing test chunk 19...\n",
      "  Processing test chunk 20...\n",
      "  Processing test chunk 21...\n",
      "  Processing test chunk 22...\n",
      "  Processing test chunk 23...\n",
      "Concatenating test chunks...\n",
      "Saving to ../data_parquet/test.parquet...\n",
      "Test data conversion complete.\n",
      "\n",
      "All conversions finished!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define where your raw CSV data is\n",
    "CSV_DATA_DIR = '../data/' # Adjust if your data folder is named differently\n",
    "\n",
    "# Define where you want to save the new Parquet files\n",
    "PARQUET_OUTPUT_DIR = '../data_parquet/' # Create this folder if it doesn't exist\n",
    "\n",
    "# Create the output directory if needed\n",
    "os.makedirs(PARQUET_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Process train_data.csv ---\n",
    "train_csv_path = os.path.join(CSV_DATA_DIR, 'train_data.csv')\n",
    "train_parquet_path = os.path.join(PARQUET_OUTPUT_DIR, 'train.parquet')\n",
    "\n",
    "if not os.path.exists(train_parquet_path):\n",
    "    print(f\"Converting {train_csv_path} to parquet...\")\n",
    "    \n",
    "    # Read the CSV in chunks and append to a temporary list\n",
    "    chunk_list = []\n",
    "    reader = pd.read_csv(train_csv_path, chunksize=500000) \n",
    "    \n",
    "    for i, chunk in enumerate(reader):\n",
    "        print(f\"  Processing train chunk {i+1}...\")\n",
    "        chunk_list.append(chunk)\n",
    "        gc.collect() # Try to free up memory after each chunk\n",
    "\n",
    "    # Concatenate all chunks into one DataFrame\n",
    "    print(\"Concatenating train chunks...\")\n",
    "    train_full_df = pd.concat(chunk_list, ignore_index=True)\n",
    "    del chunk_list # Free up memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Save the full DataFrame to Parquet\n",
    "    print(f\"Saving to {train_parquet_path}...\")\n",
    "    train_full_df.to_parquet(train_parquet_path, engine='fastparquet')\n",
    "    del train_full_df # Free up memory\n",
    "    gc.collect()\n",
    "    print(\"Train data conversion complete.\")\n",
    "else:\n",
    "    print(f\"{train_parquet_path} already exists. Skipping conversion.\")\n",
    "\n",
    "# --- Process test_data.csv ---\n",
    "test_csv_path = os.path.join(CSV_DATA_DIR, 'test_data.csv')\n",
    "test_parquet_path = os.path.join(PARQUET_OUTPUT_DIR, 'test.parquet')\n",
    "\n",
    "if not os.path.exists(test_parquet_path):\n",
    "    print(f\"\\nConverting {test_csv_path} to parquet...\")\n",
    "    \n",
    "    chunk_list = []\n",
    "    reader = pd.read_csv(test_csv_path, chunksize=500000)\n",
    "    \n",
    "    for i, chunk in enumerate(reader):\n",
    "        print(f\"  Processing test chunk {i+1}...\")\n",
    "        chunk_list.append(chunk)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"Concatenating test chunks...\")\n",
    "    test_full_df = pd.concat(chunk_list, ignore_index=True)\n",
    "    del chunk_list\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Saving to {test_parquet_path}...\")\n",
    "    test_full_df.to_parquet(test_parquet_path, engine='fastparquet')\n",
    "    del test_full_df\n",
    "    gc.collect()\n",
    "    print(\"Test data conversion complete.\")\n",
    "else:\n",
    "    print(f\"{test_parquet_path} already exists. Skipping conversion.\")\n",
    "\n",
    "print(\"\\nAll conversions finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c062d7e",
   "metadata": {},
   "source": [
    "It's pretty good, about a 60% decrease in storage from 50GB to about 20GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f8d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original training data from Parquet...\n",
      "Original Parquet loaded. Shape: (5531451, 190)\n",
      "\n",
      "Optimizing memory usage...\n",
      "Memory usage of dataframe is 8018.31 MB\n",
      "Memory usage after optimization is: 4077.73 MB\n",
      "Decreased by 49.1%\n",
      "\n",
      "Saving optimized DataFrame to ../data_parquet/train_downcasted.parquet...\n",
      "Optimized DataFrame saved.\n",
      "\n",
      "Loading training labels...\n",
      "Training labels loaded. Shape: (458913, 2)\n",
      "\n",
      "Showing first few rows of optimized training data:\n",
      "                                         customer_ID         S_2       P_2  \\\n",
      "0  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-03-09  0.938469   \n",
      "1  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-04-07  0.936665   \n",
      "2  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-05-28  0.954180   \n",
      "3  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-06-13  0.960384   \n",
      "4  0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...  2017-07-16  0.947248   \n",
      "\n",
      "       D_39       B_1       B_2       R_1       S_3      D_41       B_3  ...  \\\n",
      "0  0.001733  0.008724  1.006838  0.009228  0.124035  0.008771  0.004709  ...   \n",
      "1  0.005775  0.004923  1.000653  0.006151  0.126750  0.000798  0.002714  ...   \n",
      "2  0.091505  0.021655  1.009672  0.006815  0.123977  0.007598  0.009423  ...   \n",
      "3  0.002455  0.013683  1.002700  0.001373  0.117169  0.000685  0.005531  ...   \n",
      "4  0.002483  0.015193  1.000727  0.007605  0.117325  0.004653  0.009312  ...   \n",
      "\n",
      "   D_136  D_137  D_138     D_139     D_140     D_141  D_142     D_143  \\\n",
      "0    NaN    NaN    NaN  0.002427  0.003706  0.003818    NaN  0.000569   \n",
      "1    NaN    NaN    NaN  0.003954  0.003167  0.005032    NaN  0.009576   \n",
      "2    NaN    NaN    NaN  0.003269  0.007329  0.000427    NaN  0.003429   \n",
      "3    NaN    NaN    NaN  0.006117  0.004516  0.003200    NaN  0.008419   \n",
      "4    NaN    NaN    NaN  0.003671  0.004946  0.008889    NaN  0.001670   \n",
      "\n",
      "      D_144     D_145  \n",
      "0  0.000610  0.002674  \n",
      "1  0.005492  0.009217  \n",
      "2  0.006986  0.002603  \n",
      "3  0.006527  0.009600  \n",
      "4  0.008126  0.009827  \n",
      "\n",
      "[5 rows x 190 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Memory Reduction Function (with detailed printout) ---\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object: # Exclude object (string) columns\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else: # Float types\n",
    "                # Check for float32 first\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else: # Fallback to float64 if range is too large\n",
    "                    df[col] = df[col].astype(np.float64) \n",
    "        # else: # Optionally convert object types to category\n",
    "        #     df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "        print(f'Decreased by {100 * (start_mem - end_mem) / start_mem:.1f}%')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# --- Define File Paths ---\n",
    "PARQUET_DATA_DIR = '../data_parquet/' \n",
    "CSV_DATA_DIR = '../data/' \n",
    "\n",
    "# Define paths for original and optimized Parquet files\n",
    "train_parquet_path = os.path.join(PARQUET_DATA_DIR, 'train.parquet')\n",
    "train_downcasted_parquet_path = os.path.join(PARQUET_DATA_DIR, 'train_optimized.parquet')\n",
    "\n",
    "# --- Load Original Parquet Data ---\n",
    "print(\"Loading original training data from Parquet...\")\n",
    "train_df = pd.read_parquet(train_parquet_path, engine='fastparquet')\n",
    "print(f\"Original Parquet loaded. Shape: {train_df.shape}\")\n",
    "\n",
    "# --- Optimize Memory Usage ---\n",
    "# The function will now print the before/after memory\n",
    "print(\"\\nOptimizing memory usage...\")\n",
    "train_df_optimized = reduce_mem_usage(train_df) \n",
    "\n",
    "# --- Save the Optimized DataFrame ---\n",
    "# Check if it doesn't already exist to avoid re-saving unnecessarily\n",
    "if not os.path.exists(train_downcasted_parquet_path):\n",
    "    print(f\"\\nSaving optimized DataFrame to {train_downcasted_parquet_path}...\")\n",
    "    train_df_optimized.to_parquet(train_downcasted_parquet_path, engine='fastparquet')\n",
    "    print(\"Optimized DataFrame saved.\")\n",
    "else:\n",
    "    print(f\"\\nOptimized file {train_downcasted_parquet_path} already exists. Skipping save.\")\n",
    "\n",
    "# --- Load Training Labels ---\n",
    "print(\"\\nLoading training labels...\")\n",
    "train_labels_path = os.path.join(CSV_DATA_DIR, 'train_labels.csv')\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "print(f\"Training labels loaded. Shape: {train_labels.shape}\")\n",
    "\n",
    "# --- Clean up original large DataFrame from memory ---\n",
    "del train_df \n",
    "gc.collect()\n",
    "\n",
    "# --- Display Head of Optimi,,,,,,,,,zed Data ---\n",
    "print(\"\\nShowing first few rows of optimized training data:\")\n",
    "print(train_df_optimized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf91685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original test data from ../data_parquet/test.parquet...\n",
      "\n",
      "Optimizing test data memory usage...\n",
      "Memory usage of dataframe is 16472.74 MB\n",
      "Memory usage after optimization is: 8377.25 MB\n",
      "Decreased by 49.1%\n",
      "\n",
      "Saving optimized test data to ../data_parquet/test_optimized.parquet...\n",
      "\n",
      "Test data optimization complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Define File Paths ---\n",
    "ORIGINAL_TEST_PATH = os.path.join(PARQUET_DATA_DIR, 'test.parquet')\n",
    "OPTIMIZED_TEST_PATH = os.path.join(PARQUET_DATA_DIR, 'test_optimized.parquet') # New file!\n",
    "\n",
    "# --- Load, Optimize, and Save Test Data ---\n",
    "if not os.path.exists(OPTIMIZED_TEST_PATH):\n",
    "    print(f\"Loading original test data from {ORIGINAL_TEST_PATH}...\")\n",
    "    test_df = pd.read_parquet(ORIGINAL_TEST_PATH, engine='fastparquet')\n",
    "\n",
    "    print(\"\\nOptimizing test data memory usage...\")\n",
    "    test_df_optimized = reduce_mem_usage(test_df)\n",
    "    del test_df # Free up memory\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"\\nSaving optimized test data to {OPTIMIZED_TEST_PATH}...\")\n",
    "    test_df_optimized.to_parquet(OPTIMIZED_TEST_PATH, engine ='fastparquet') \n",
    "    del test_df_optimized # Free up memory\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\nTest data optimization complete.\")\n",
    "else:\n",
    "    print(f\"{OPTIMIZED_TEST_PATH} already exists. Skipping.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
