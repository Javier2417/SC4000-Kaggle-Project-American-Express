{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Inference: LightGBM + CatBoost Test Predictions (MEMORY-EFFICIENT)\n",
        "\n",
        "**Memory Issue Fixed:**\n",
        "- The test dataset is too large (7005 samples × 924621 features = 48.3 GB)\n",
        "- This notebook processes predictions in **batches** to avoid memory errors\n",
        "\n",
        "**What this does:**\n",
        "1. Load test data in chunks\n",
        "2. Generate predictions batch-by-batch\n",
        "3. Combine results efficiently\n",
        "4. Create ensemble submissions\n",
        "\n",
        "**Memory-efficient approach:**\n",
        "- Batch size: 500 samples at a time\n",
        "- Aggressive garbage collection\n",
        "- Sequential processing to minimize RAM usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM version: 4.6.0\n",
            "CatBoost version: 1.2.8\n",
            "\n",
            "Batch size: 500 samples\n",
            "This will prevent memory errors by processing data in chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(f\"LightGBM version: {lgb.__version__}\")\n",
        "print(f\"CatBoost version: {cb.__version__}\")\n",
        "\n",
        "# --- Define Paths ---\n",
        "FE_DATA_DIR = '../data_fe/'\n",
        "CSV_DATA_DIR = '../data/'\n",
        "MODEL_DIR = './models/'\n",
        "PREPROCESSOR_DIR = './preprocessors/'\n",
        "\n",
        "TRAIN_PATH = os.path.join(FE_DATA_DIR, 'train_processed.parquet')\n",
        "TEST_PATH = os.path.join(FE_DATA_DIR, 'test_processed.parquet') \n",
        "SUB_PATH = os.path.join(CSV_DATA_DIR, 'sample_submission.csv')\n",
        "CB_MODEL_DIR = os.path.join(MODEL_DIR, 'catboost')\n",
        "\n",
        "# Training configuration\n",
        "SEEDS = [42, 52, 62]\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 500  # Process 500 samples at a time to avoid memory errors\n",
        "\n",
        "print(f\"\\nBatch size: {BATCH_SIZE} samples\")\n",
        "print(\"This will prevent memory errors by processing data in chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "load_config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading column configuration...\n",
            "Total features: 7005\n",
            "Categorical features: 99\n"
          ]
        }
      ],
      "source": [
        "# Load column configuration\n",
        "print(\"Loading column configuration...\")\n",
        "with open(os.path.join(PREPROCESSOR_DIR, 'column_lists.json'), 'r') as f:\n",
        "    column_lists = json.load(f)\n",
        "\n",
        "features = column_lists['all_features']\n",
        "categorical_cols = [col for col in column_lists['categorical_cols_for_lgb'] if col in features]\n",
        "\n",
        "print(f\"Total features: {len(features)}\")\n",
        "print(f\"Categorical features: {len(categorical_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_test",
      "metadata": {},
      "source": [
        "## Step 1: Load Test Data (with customer IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "load_test_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING TEST DATA\n",
            "======================================================================\n",
            "\n",
            "Loading test data from ../data_fe/test_processed.parquet...\n",
            "Test data loaded in 53.00s\n",
            "Test shape: (924621, 7006)\n",
            "Memory usage: 13.52 GB\n",
            "\n",
            "Total samples: 924,621\n",
            "Batch size: 500\n",
            "Number of batches: 1850\n",
            "Features shape: (924621, 7005)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING TEST DATA\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"Loading test data from {TEST_PATH}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load only customer_ID and features columns to save memory\n",
        "cols_to_load = ['customer_ID'] + features\n",
        "X_test = pd.read_parquet(TEST_PATH, columns=cols_to_load)\n",
        "\n",
        "print(f\"Test data loaded in {time.time() - start_time:.2f}s\")\n",
        "print(f\"Test shape: {X_test.shape}\")\n",
        "print(f\"Memory usage: {X_test.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
        "\n",
        "# Store customer IDs separately\n",
        "customer_ids = X_test['customer_ID'].copy()\n",
        "X_test_features = X_test[features]\n",
        "del X_test\n",
        "gc.collect()\n",
        "\n",
        "n_samples = len(X_test_features)\n",
        "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "print(f\"\\nTotal samples: {n_samples:,}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Number of batches: {n_batches}\")\n",
        "print(f\"Features shape: {X_test_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lgbm_inference",
      "metadata": {},
      "source": [
        "## Step 2: LightGBM Test Predictions (Batch Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "lgbm_test_preds",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING LIGHTGBM TEST PREDICTIONS (BATCH MODE)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 42\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:57<00:00,  7.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:54<00:00,  7.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:58<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:59<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:57<00:00,  7.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 42 avg prediction: 0.249028\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 52\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:58<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:04<00:00,  7.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:05<00:00,  7.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:02<00:00,  7.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [05:17<00:00,  5.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 52 avg prediction: 0.248813\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 62\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:40<00:00,  6.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:42<00:00,  6.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 62 avg prediction: 0.249149\n",
            "\n",
            "======================================================================\n",
            "LightGBM predictions complete\n",
            "  Shape: (924621,)\n",
            "  Range: [0.000048, 0.999842]\n",
            "  Mean: 0.248997\n",
            "======================================================================\n",
            "\n",
            "✓ Saved: ./models/test_preds_lgbm.npy\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING LIGHTGBM TEST PREDICTIONS (BATCH MODE)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Initialize predictions array\n",
        "test_preds_lgb = np.zeros(n_samples, dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing LightGBM seed {seed}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    seed_preds = np.zeros(n_samples, dtype=np.float32)\n",
        "    \n",
        "    for fold in range(N_SPLITS):\n",
        "        print(f\"\\n  Fold {fold+1}/{N_SPLITS}\")\n",
        "        model_path = os.path.join(MODEL_DIR, f'model_seed_{seed}_fold_{fold}.txt')\n",
        "        \n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"    ✗ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Load model once\n",
        "        model = lgb.Booster(model_file=model_path)\n",
        "        print(f\"    Model loaded from: {model_path}\")\n",
        "        \n",
        "        # Process in batches\n",
        "        for batch_idx in tqdm(range(n_batches), desc=f\"    Predicting\"):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = min((batch_idx + 1) * BATCH_SIZE, n_samples)\n",
        "            \n",
        "            # Get batch data\n",
        "            batch_data = X_test_features.iloc[start_idx:end_idx]\n",
        "            \n",
        "            # Predict on batch (convert to numpy to avoid pandas memory issues)\n",
        "            batch_preds = model.predict(batch_data.values)\n",
        "            seed_preds[start_idx:end_idx] += batch_preds / N_SPLITS\n",
        "            \n",
        "            del batch_data, batch_preds\n",
        "            gc.collect()\n",
        "        \n",
        "        del model\n",
        "        gc.collect()\n",
        "        print(f\"    ✓ Fold {fold+1} complete\")\n",
        "    \n",
        "    # Add seed predictions to overall predictions\n",
        "    test_preds_lgb += seed_preds / len(SEEDS)\n",
        "    print(f\"\\n  Seed {seed} avg prediction: {seed_preds.mean():.6f}\")\n",
        "    del seed_preds\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"LightGBM predictions complete\")\n",
        "print(f\"  Shape: {test_preds_lgb.shape}\")\n",
        "print(f\"  Range: [{test_preds_lgb.min():.6f}, {test_preds_lgb.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_lgb.mean():.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save predictions\n",
        "np.save(os.path.join(MODEL_DIR, 'test_preds_lgbm.npy'), test_preds_lgb)\n",
        "print(f\"\\n✓ Saved: {os.path.join(MODEL_DIR, 'test_preds_lgbm.npy')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "catboost_inference",
      "metadata": {},
      "source": [
        "## Step 3: CatBoost Test Predictions (Batch Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "catboost_test_preds",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING CATBOOST TEST PREDICTIONS (BATCH MODE)\n",
            "======================================================================\n",
            "\n",
            "Preparing categorical features for CatBoost...\n",
            "✓ Categorical features converted to string type\n",
            "\n",
            "======================================================================\n",
            "Processing CatBoost seed 42\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_0.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [32:55<00:00,  1.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_1.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [50:26<00:00,  1.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_2.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [42:18<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_3.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [31:36<00:00,  1.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_4.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [28:44<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 42 avg prediction: 0.250170\n",
            "======================================================================\n",
            "Processing CatBoost seed 52\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_0.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [26:40<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_1.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting:  36%|███▌      | 662/1850 [09:41<16:52,  1.17it/s]"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING CATBOOST TEST PREDICTIONS (BATCH MODE)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Prepare categorical features (convert to string)\n",
        "print(\"Preparing categorical features for CatBoost...\")\n",
        "X_test_cb = X_test_features.copy()\n",
        "for col in categorical_cols:\n",
        "    if col in X_test_cb.columns:\n",
        "        X_test_cb[col] = X_test_cb[col].fillna(-999).astype(str)\n",
        "print(\"✓ Categorical features converted to string type\\n\")\n",
        "\n",
        "# Initialize predictions array\n",
        "test_preds_cb = np.zeros(n_samples, dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Processing CatBoost seed {seed}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    seed_preds = np.zeros(n_samples, dtype=np.float32)\n",
        "    \n",
        "    for fold in range(N_SPLITS):\n",
        "        print(f\"\\n  Fold {fold+1}/{N_SPLITS}\")\n",
        "        model_path = os.path.join(CB_MODEL_DIR, f'catboost_seed_{seed}_fold_{fold}.cbm')\n",
        "        \n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"    ✗ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Load model once\n",
        "        model = cb.CatBoostClassifier()\n",
        "        model.load_model(model_path)\n",
        "        print(f\"    Model loaded from: {model_path}\")\n",
        "        \n",
        "        # Process in batches\n",
        "        for batch_idx in tqdm(range(n_batches), desc=f\"    Predicting\"):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = min((batch_idx + 1) * BATCH_SIZE, n_samples)\n",
        "            \n",
        "            # Get batch data\n",
        "            batch_data = X_test_cb.iloc[start_idx:end_idx]\n",
        "            \n",
        "            # Predict on batch\n",
        "            batch_preds = model.predict_proba(batch_data)[:, 1]\n",
        "            seed_preds[start_idx:end_idx] += batch_preds / N_SPLITS\n",
        "            \n",
        "            del batch_data, batch_preds\n",
        "            gc.collect()\n",
        "        \n",
        "        del model\n",
        "        gc.collect()\n",
        "        print(f\"    ✓ Fold {fold+1} complete\")\n",
        "    \n",
        "    # Add seed predictions to overall predictions\n",
        "    test_preds_cb += seed_preds / len(SEEDS)\n",
        "    print(f\"\\n  Seed {seed} avg prediction: {seed_preds.mean():.6f}\")\n",
        "    del seed_preds\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"CatBoost predictions complete\")\n",
        "print(f\"  Shape: {test_preds_cb.shape}\")\n",
        "print(f\"  Range: [{test_preds_cb.min():.6f}, {test_preds_cb.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_cb.mean():.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save predictions\n",
        "np.save(os.path.join(MODEL_DIR, 'test_preds_catboost.npy'), test_preds_cb)\n",
        "print(f\"\\n✓ Saved: {os.path.join(MODEL_DIR, 'test_preds_catboost.npy')}\")\n",
        "\n",
        "# Clean up\n",
        "del X_test_cb, X_test_features\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ensemble_weights",
      "metadata": {},
      "source": [
        "## Step 4: Find Optimal Ensemble Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optimal_weights",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINDING OPTIMAL ENSEMBLE WEIGHTS\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Try to load OOF predictions for weight optimization\n",
        "oof_lgb_path = os.path.join(MODEL_DIR, 'oof_lgbm.npy')\n",
        "oof_cb_path = os.path.join(MODEL_DIR, 'oof_catboost.npy')\n",
        "\n",
        "best_weights = (0.5, 0.5)  # Default\n",
        "\n",
        "if os.path.exists(oof_cb_path):\n",
        "    try:\n",
        "        from amex_metric import amex_metric\n",
        "        \n",
        "        # Load training target\n",
        "        train_df = pd.read_parquet(TRAIN_PATH, columns=['target'])\n",
        "        y_train = train_df['target']\n",
        "        del train_df\n",
        "        gc.collect()\n",
        "        \n",
        "        # Load OOF predictions\n",
        "        if os.path.exists(oof_lgb_path):\n",
        "            oof_lgb = np.load(oof_lgb_path)\n",
        "        else:\n",
        "            oof_lgb = None\n",
        "        \n",
        "        oof_cb = np.load(oof_cb_path)\n",
        "        \n",
        "        if oof_lgb is not None:\n",
        "            def amex_metric_mod(y_true, y_pred):\n",
        "                dummy_index = range(len(y_true))\n",
        "                y_true_df = pd.DataFrame({'target': y_true}, index=dummy_index)\n",
        "                y_pred_df = pd.DataFrame({'prediction': y_pred}, index=dummy_index)\n",
        "                y_true_df.index.name = 'customer_ID'\n",
        "                y_pred_df.index.name = 'customer_ID'\n",
        "                return amex_metric(y_true_df, y_pred_df)\n",
        "            \n",
        "            print(\"Testing ensemble weights...\\n\")\n",
        "            best_score = 0\n",
        "            \n",
        "            weight_combinations = [\n",
        "                (0.5, 0.5), (0.6, 0.4), (0.7, 0.3), (0.4, 0.6), (0.3, 0.7),\n",
        "                (0.55, 0.45), (0.45, 0.55), (0.65, 0.35), (0.35, 0.65)\n",
        "            ]\n",
        "            \n",
        "            for w_lgb, w_cb in weight_combinations:\n",
        "                oof_ensemble = w_lgb * oof_lgb + w_cb * oof_cb\n",
        "                score = amex_metric_mod(y_train.values, oof_ensemble)\n",
        "                print(f\"  Weights (LGB={w_lgb:.2f}, CB={w_cb:.2f}): Score = {score:.6f}\")\n",
        "                \n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_weights = (w_lgb, w_cb)\n",
        "            \n",
        "            print(f\"\\n✓ Best weights: LGB={best_weights[0]:.3f}, CB={best_weights[1]:.3f}\")\n",
        "            print(f\"✓ Best OOF score: {best_score:.6f}\")\n",
        "        else:\n",
        "            print(\"LightGBM OOF not found, using equal weights (0.5, 0.5)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not optimize weights: {e}\")\n",
        "        print(\"Using default equal weights (0.5, 0.5)\")\n",
        "else:\n",
        "    print(\"OOF predictions not found, using default equal weights (0.5, 0.5)\")\n",
        "\n",
        "print(f\"\\nFinal ensemble weights: LGB={best_weights[0]:.3f}, CB={best_weights[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create_submissions",
      "metadata": {},
      "source": [
        "## Step 5: Create Submission Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "submissions",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING SUBMISSION FILES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load sample submission\n",
        "sample_sub = pd.read_csv(SUB_PATH)\n",
        "print(f\"Sample submission loaded: {sample_sub.shape}\")\n",
        "\n",
        "# Create ensemble predictions\n",
        "test_preds_ensemble = best_weights[0] * test_preds_lgb + best_weights[1] * test_preds_cb\n",
        "\n",
        "print(f\"\\nEnsemble predictions:\")\n",
        "print(f\"  Shape: {test_preds_ensemble.shape}\")\n",
        "print(f\"  Range: [{test_preds_ensemble.min():.6f}, {test_preds_ensemble.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_ensemble.mean():.6f}\")\n",
        "\n",
        "# 1. Ensemble submission\n",
        "submission_ensemble = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_ensemble\n",
        "})\n",
        "submission_ensemble = sample_sub[['customer_ID']].merge(submission_ensemble, on='customer_ID', how='left')\n",
        "submission_ensemble['prediction'] = submission_ensemble['prediction'].fillna(0.0)\n",
        "ensemble_path = 'submission_lgbm_catboost_ensemble.csv'\n",
        "submission_ensemble.to_csv(ensemble_path, index=False)\n",
        "print(f\"\\n✓ Ensemble submission saved: {ensemble_path}\")\n",
        "\n",
        "# 2. LightGBM only\n",
        "submission_lgbm = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_lgb\n",
        "})\n",
        "submission_lgbm = sample_sub[['customer_ID']].merge(submission_lgbm, on='customer_ID', how='left')\n",
        "submission_lgbm['prediction'] = submission_lgbm['prediction'].fillna(0.0)\n",
        "lgbm_path = 'submission_lgbm_only.csv'\n",
        "submission_lgbm.to_csv(lgbm_path, index=False)\n",
        "print(f\"✓ LightGBM submission saved: {lgbm_path}\")\n",
        "\n",
        "# 3. CatBoost only\n",
        "submission_cb = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_cb\n",
        "})\n",
        "submission_cb = sample_sub[['customer_ID']].merge(submission_cb, on='customer_ID', how='left')\n",
        "submission_cb['prediction'] = submission_cb['prediction'].fillna(0.0)\n",
        "cb_path = 'submission_catboost_only.csv'\n",
        "submission_cb.to_csv(cb_path, index=False)\n",
        "print(f\"✓ CatBoost submission saved: {cb_path}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✅ ALL SUBMISSIONS CREATED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nPreview of ensemble submission:\")\n",
        "print(submission_ensemble.head(10))\n",
        "print(f\"\\nSubmission statistics:\")\n",
        "print(submission_ensemble['prediction'].describe())\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Memory-Efficient Processing\n",
        "- ✅ Processed test data in batches of 500 samples\n",
        "- ✅ Avoided the 48.3 GB memory allocation error\n",
        "- ✅ Used numpy arrays instead of pandas DataFrames where possible\n",
        "- ✅ Aggressive garbage collection after each batch\n",
        "\n",
        "### Files Created\n",
        "1. **submission_lgbm_catboost_ensemble.csv** - Ensemble (RECOMMENDED)\n",
        "2. **submission_lgbm_only.csv** - LightGBM only\n",
        "3. **submission_catboost_only.csv** - CatBoost only\n",
        "4. **models/test_preds_lgbm.npy** - Saved predictions\n",
        "5. **models/test_preds_catboost.npy** - Saved predictions\n",
        "\n",
        "### Models Used\n",
        "- LightGBM: 15 models (3 seeds × 5 folds)\n",
        "- CatBoost: 15 models (3 seeds × 5 folds)\n",
        "- Total: 30 models averaged\n",
        "\n",
        "### Performance Tips\n",
        "- If still getting memory errors, reduce BATCH_SIZE to 250 or 100\n",
        "- The batch processing adds ~5-10 minutes to total runtime\n",
        "- Monitor memory usage in Task Manager during execution"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
