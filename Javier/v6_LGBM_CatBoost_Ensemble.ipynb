{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Inference: LightGBM + CatBoost Test Predictions (MEMORY-EFFICIENT)\n",
        "\n",
        "**Memory Issue Fixed:**\n",
        "- The test dataset is too large (7005 samples × 924621 features = 48.3 GB)\n",
        "- This notebook processes predictions in **batches** to avoid memory errors\n",
        "\n",
        "**What this does:**\n",
        "1. Load test data in chunks\n",
        "2. Generate predictions batch-by-batch\n",
        "3. Combine results efficiently\n",
        "4. Create ensemble submissions\n",
        "\n",
        "**Memory-efficient approach:**\n",
        "- Batch size: 500 samples at a time\n",
        "- Aggressive garbage collection\n",
        "- Sequential processing to minimize RAM usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "imports",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM version: 4.6.0\n",
            "CatBoost version: 1.2.8\n",
            "\n",
            "Batch size: 500 samples\n",
            "This will prevent memory errors by processing data in chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import gc\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(f\"LightGBM version: {lgb.__version__}\")\n",
        "print(f\"CatBoost version: {cb.__version__}\")\n",
        "\n",
        "# --- Define Paths ---\n",
        "FE_DATA_DIR = '../data_fe/'\n",
        "CSV_DATA_DIR = '../data/'\n",
        "MODEL_DIR = './models/'\n",
        "PREPROCESSOR_DIR = './preprocessors/'\n",
        "\n",
        "TRAIN_PATH = os.path.join(FE_DATA_DIR, 'train_processed.parquet')\n",
        "TEST_PATH = os.path.join(FE_DATA_DIR, 'test_processed.parquet') \n",
        "SUB_PATH = os.path.join(CSV_DATA_DIR, 'sample_submission.csv')\n",
        "CB_MODEL_DIR = os.path.join(MODEL_DIR, 'catboost')\n",
        "\n",
        "# Training configuration\n",
        "SEEDS = [42, 52, 62]\n",
        "N_SPLITS = 5\n",
        "BATCH_SIZE = 500  # Process 500 samples at a time to avoid memory errors\n",
        "\n",
        "print(f\"\\nBatch size: {BATCH_SIZE} samples\")\n",
        "print(\"This will prevent memory errors by processing data in chunks.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "load_config",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading column configuration...\n",
            "Total features: 7005\n",
            "Categorical features: 99\n"
          ]
        }
      ],
      "source": [
        "# Load column configuration\n",
        "print(\"Loading column configuration...\")\n",
        "with open(os.path.join(PREPROCESSOR_DIR, 'column_lists.json'), 'r') as f:\n",
        "    column_lists = json.load(f)\n",
        "\n",
        "features = column_lists['all_features']\n",
        "categorical_cols = [col for col in column_lists['categorical_cols_for_lgb'] if col in features]\n",
        "\n",
        "print(f\"Total features: {len(features)}\")\n",
        "print(f\"Categorical features: {len(categorical_cols)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load_test",
      "metadata": {},
      "source": [
        "## Step 1: Load Test Data (with customer IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "load_test_data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "LOADING TEST DATA\n",
            "======================================================================\n",
            "\n",
            "Loading test data from ../data_fe/test_processed.parquet...\n",
            "Test data loaded in 92.39s\n",
            "Test shape: (924621, 7006)\n",
            "Memory usage: 13.52 GB\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 11.9 GiB for an array with shape (6906, 924621) and data type float16",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Store customer IDs separately\u001b[39;00m\n\u001b[32m     17\u001b[39m customer_ids = X_test[\u001b[33m'\u001b[39m\u001b[33mcustomer_ID\u001b[39m\u001b[33m'\u001b[39m].copy()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m X_test_features = \u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m X_test\n\u001b[32m     20\u001b[39m gc.collect()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4128\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m   4126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._slice(indexer, axis=\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4128\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   4130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[32m   4131\u001b[39m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[32m   4132\u001b[39m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[32m   4133\u001b[39m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[32m   4134\u001b[39m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[32m   4135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.columns, MultiIndex):\n\u001b[32m   4136\u001b[39m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4175\u001b[39m, in \u001b[36mNDFrame._take_with_is_copy\u001b[39m\u001b[34m(self, indices, axis)\u001b[39m\n\u001b[32m   4164\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   4165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis = \u001b[32m0\u001b[39m) -> Self:\n\u001b[32m   4166\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4167\u001b[39m \u001b[33;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[32m   4168\u001b[39m \u001b[33;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4173\u001b[39m \u001b[33;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[32m   4174\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4175\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4176\u001b[39m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[32m   4177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result._get_axis(axis).equals(\u001b[38;5;28mself\u001b[39m._get_axis(axis)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:4155\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4150\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4151\u001b[39m     indices = np.arange(\n\u001b[32m   4152\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4153\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4161\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4162\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:913\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    910\u001b[39m indexer = maybe_convert_indices(indexer, n, verify=verify)\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:699\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    706\u001b[39m     new_blocks = [\n\u001b[32m    707\u001b[39m         blk.take_nd(\n\u001b[32m    708\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    714\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    715\u001b[39m     ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:862\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    860\u001b[39m                     blocks.append(nb)\n\u001b[32m    861\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    863\u001b[39m                 blocks.append(nb)\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1373\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1370\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1372\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1378\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1380\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1381\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:345\u001b[39m, in \u001b[36m_get_take_nd_function.<locals>.func\u001b[39m\u001b[34m(arr, indexer, out, fill_value)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(arr, indexer, out, fill_value=np.nan) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    344\u001b[39m     indexer = ensure_platform_int(indexer)\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     \u001b[43m_take_nd_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_info\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\OneDrive - Nanyang Technological University\\NTU School Work\\Year 3 S1\\SC4000\\Project\\amex-default-prediction-project-code\\.venv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:528\u001b[39m, in \u001b[36m_take_nd_object\u001b[39m\u001b[34m(arr, indexer, out, axis, fill_value, mask_info)\u001b[39m\n\u001b[32m    526\u001b[39m     arr = arr.astype(out.dtype)\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.shape[axis] > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_masking:\n\u001b[32m    530\u001b[39m     outindexer = [\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)] * arr.ndim\n",
            "\u001b[31mMemoryError\u001b[39m: Unable to allocate 11.9 GiB for an array with shape (6906, 924621) and data type float16"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LOADING TEST DATA\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(f\"Loading test data from {TEST_PATH}...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load only customer_ID and features columns to save memory\n",
        "cols_to_load = ['customer_ID'] + features\n",
        "X_test = pd.read_parquet(TEST_PATH, columns=cols_to_load)\n",
        "\n",
        "print(f\"Test data loaded in {time.time() - start_time:.2f}s\")\n",
        "print(f\"Test shape: {X_test.shape}\")\n",
        "print(f\"Memory usage: {X_test.memory_usage(deep=True).sum() / 1e9:.2f} GB\")\n",
        "\n",
        "# Store customer IDs separately\n",
        "customer_ids = X_test['customer_ID'].copy()\n",
        "X_test_features = X_test[features]\n",
        "del X_test\n",
        "gc.collect()\n",
        "\n",
        "n_samples = len(X_test_features)\n",
        "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "print(f\"\\nTotal samples: {n_samples:,}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Number of batches: {n_batches}\")\n",
        "print(f\"Features shape: {X_test_features.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lgbm_inference",
      "metadata": {},
      "source": [
        "## Step 2: LightGBM Test Predictions (Batch Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "lgbm_test_preds",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING LIGHTGBM TEST PREDICTIONS (BATCH MODE)\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 42\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:57<00:00,  7.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:54<00:00,  7.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:58<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:59<00:00,  7.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_42_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:57<00:00,  7.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 42 avg prediction: 0.249028\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 52\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [03:58<00:00,  7.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:04<00:00,  7.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:05<00:00,  7.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:02<00:00,  7.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_52_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [05:17<00:00,  5.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 52 avg prediction: 0.248813\n",
            "\n",
            "======================================================================\n",
            "Processing LightGBM seed 62\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_0.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:40<00:00,  6.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_1.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_2.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_3.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:39<00:00,  6.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/model_seed_62_fold_4.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [04:42<00:00,  6.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 62 avg prediction: 0.249149\n",
            "\n",
            "======================================================================\n",
            "LightGBM predictions complete\n",
            "  Shape: (924621,)\n",
            "  Range: [0.000048, 0.999842]\n",
            "  Mean: 0.248997\n",
            "======================================================================\n",
            "\n",
            "✓ Saved: ./models/test_preds_lgbm.npy\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING LIGHTGBM TEST PREDICTIONS (BATCH MODE)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Initialize predictions array\n",
        "test_preds_lgb = np.zeros(n_samples, dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing LightGBM seed {seed}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    seed_preds = np.zeros(n_samples, dtype=np.float32)\n",
        "    \n",
        "    for fold in range(N_SPLITS):\n",
        "        print(f\"\\n  Fold {fold+1}/{N_SPLITS}\")\n",
        "        model_path = os.path.join(MODEL_DIR, f'model_seed_{seed}_fold_{fold}.txt')\n",
        "        \n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"    ✗ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Load model once\n",
        "        model = lgb.Booster(model_file=model_path)\n",
        "        print(f\"    Model loaded from: {model_path}\")\n",
        "        \n",
        "        # Process in batches\n",
        "        for batch_idx in tqdm(range(n_batches), desc=f\"    Predicting\"):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = min((batch_idx + 1) * BATCH_SIZE, n_samples)\n",
        "            \n",
        "            # Get batch data\n",
        "            batch_data = X_test_features.iloc[start_idx:end_idx]\n",
        "            \n",
        "            # Predict on batch (convert to numpy to avoid pandas memory issues)\n",
        "            batch_preds = model.predict(batch_data.values)\n",
        "            seed_preds[start_idx:end_idx] += batch_preds / N_SPLITS\n",
        "            \n",
        "            del batch_data, batch_preds\n",
        "            gc.collect()\n",
        "        \n",
        "        del model\n",
        "        gc.collect()\n",
        "        print(f\"    ✓ Fold {fold+1} complete\")\n",
        "    \n",
        "    # Add seed predictions to overall predictions\n",
        "    test_preds_lgb += seed_preds / len(SEEDS)\n",
        "    print(f\"\\n  Seed {seed} avg prediction: {seed_preds.mean():.6f}\")\n",
        "    del seed_preds\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"LightGBM predictions complete\")\n",
        "print(f\"  Shape: {test_preds_lgb.shape}\")\n",
        "print(f\"  Range: [{test_preds_lgb.min():.6f}, {test_preds_lgb.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_lgb.mean():.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save predictions\n",
        "np.save(os.path.join(MODEL_DIR, 'test_preds_lgbm.npy'), test_preds_lgb)\n",
        "print(f\"\\n✓ Saved: {os.path.join(MODEL_DIR, 'test_preds_lgbm.npy')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "catboost_inference",
      "metadata": {},
      "source": [
        "## Step 3: CatBoost Test Predictions (Batch Processing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "catboost_test_preds",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GENERATING CATBOOST TEST PREDICTIONS (BATCH MODE)\n",
            "======================================================================\n",
            "\n",
            "Preparing categorical features for CatBoost...\n",
            "✓ Categorical features converted to string type\n",
            "\n",
            "======================================================================\n",
            "Processing CatBoost seed 42\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_0.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [32:55<00:00,  1.07s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_1.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [50:26<00:00,  1.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_2.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [42:18<00:00,  1.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_3.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [31:36<00:00,  1.03s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_42_fold_4.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [28:44<00:00,  1.07it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 42 avg prediction: 0.250170\n",
            "======================================================================\n",
            "Processing CatBoost seed 52\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_0.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [26:40<00:00,  1.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_1.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [38:59<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_2.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [50:31<00:00,  1.64s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_3.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [55:54<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_52_fold_4.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [46:30<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 52 avg prediction: 0.249991\n",
            "======================================================================\n",
            "Processing CatBoost seed 62\n",
            "======================================================================\n",
            "\n",
            "  Fold 1/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_62_fold_0.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [54:33<00:00,  1.77s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 1 complete\n",
            "\n",
            "  Fold 2/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_62_fold_1.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [56:17<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 2 complete\n",
            "\n",
            "  Fold 3/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_62_fold_2.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [1:00:47<00:00,  1.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 3 complete\n",
            "\n",
            "  Fold 4/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_62_fold_3.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [54:05<00:00,  1.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 4 complete\n",
            "\n",
            "  Fold 5/5\n",
            "    Model loaded from: ./models/catboost\\catboost_seed_62_fold_4.cbm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "    Predicting: 100%|██████████| 1850/1850 [55:41<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ✓ Fold 5 complete\n",
            "\n",
            "  Seed 62 avg prediction: 0.249895\n",
            "\n",
            "======================================================================\n",
            "CatBoost predictions complete\n",
            "  Shape: (924621,)\n",
            "  Range: [0.000093, 0.999978]\n",
            "  Mean: 0.250019\n",
            "======================================================================\n",
            "\n",
            "✓ Saved: ./models/test_preds_catboost.npy\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING CATBOOST TEST PREDICTIONS (BATCH MODE)\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Prepare categorical features (convert to string)\n",
        "print(\"Preparing categorical features for CatBoost...\")\n",
        "X_test_cb = X_test_features.copy()\n",
        "for col in categorical_cols:\n",
        "    if col in X_test_cb.columns:\n",
        "        X_test_cb[col] = X_test_cb[col].fillna(-999).astype(str)\n",
        "print(\"✓ Categorical features converted to string type\\n\")\n",
        "\n",
        "# Initialize predictions array\n",
        "test_preds_cb = np.zeros(n_samples, dtype=np.float32)\n",
        "\n",
        "for seed in SEEDS:\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Processing CatBoost seed {seed}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    seed_preds = np.zeros(n_samples, dtype=np.float32)\n",
        "    \n",
        "    for fold in range(N_SPLITS):\n",
        "        print(f\"\\n  Fold {fold+1}/{N_SPLITS}\")\n",
        "        model_path = os.path.join(CB_MODEL_DIR, f'catboost_seed_{seed}_fold_{fold}.cbm')\n",
        "        \n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"    ✗ Model not found: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        # Load model once\n",
        "        model = cb.CatBoostClassifier()\n",
        "        model.load_model(model_path)\n",
        "        print(f\"    Model loaded from: {model_path}\")\n",
        "        \n",
        "        # Process in batches\n",
        "        for batch_idx in tqdm(range(n_batches), desc=f\"    Predicting\"):\n",
        "            start_idx = batch_idx * BATCH_SIZE\n",
        "            end_idx = min((batch_idx + 1) * BATCH_SIZE, n_samples)\n",
        "            \n",
        "            # Get batch data\n",
        "            batch_data = X_test_cb.iloc[start_idx:end_idx]\n",
        "            \n",
        "            # Predict on batch\n",
        "            batch_preds = model.predict_proba(batch_data)[:, 1]\n",
        "            seed_preds[start_idx:end_idx] += batch_preds / N_SPLITS\n",
        "            \n",
        "            del batch_data, batch_preds\n",
        "            gc.collect()\n",
        "        \n",
        "        del model\n",
        "        gc.collect()\n",
        "        print(f\"    ✓ Fold {fold+1} complete\")\n",
        "    \n",
        "    # Add seed predictions to overall predictions\n",
        "    test_preds_cb += seed_preds / len(SEEDS)\n",
        "    print(f\"\\n  Seed {seed} avg prediction: {seed_preds.mean():.6f}\")\n",
        "    del seed_preds\n",
        "    gc.collect()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"CatBoost predictions complete\")\n",
        "print(f\"  Shape: {test_preds_cb.shape}\")\n",
        "print(f\"  Range: [{test_preds_cb.min():.6f}, {test_preds_cb.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_cb.mean():.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save predictions\n",
        "np.save(os.path.join(MODEL_DIR, 'test_preds_catboost.npy'), test_preds_cb)\n",
        "print(f\"\\n✓ Saved: {os.path.join(MODEL_DIR, 'test_preds_catboost.npy')}\")\n",
        "\n",
        "# Clean up\n",
        "del X_test_cb, X_test_features\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ensemble_weights",
      "metadata": {},
      "source": [
        "## Step 4: Find Optimal Ensemble Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "optimal_weights",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "STACKED GENERALIZATION: META-LEARNER APPROACH\n",
            "================================================================================\n",
            "\n",
            "METHOD: Two-Level Stacking Ensemble\n",
            "--------------------------------------\n",
            "Level 0: Base Models (LightGBM, CatBoost)\n",
            "Level 1: Meta-Learner (Logistic Regression with constraints)\n",
            "\n",
            "This approach learns optimal weights through cross-validation,\n",
            "avoiding overfitting while maximizing predictive performance.\n",
            "\n",
            "================================================================================\n",
            "STEP 1: PREPARE META-FEATURES\n",
            "================================================================================\n",
            "\n",
            "Meta-features shape: (458913, 2)\n",
            "  Feature 1: LightGBM predictions\n",
            "  Feature 2: CatBoost predictions\n",
            "Target shape: (458913,)\n",
            "\n",
            "Base Model Performance:\n",
            "  LightGBM: 0.800069\n",
            "  CatBoost: 0.797539\n",
            "\n",
            "================================================================================\n",
            "STEP 2: TRAIN META-LEARNER WITH CROSS-VALIDATION\n",
            "================================================================================\n",
            "\n",
            "Training meta-learner with 5-fold CV...\n",
            "\n",
            "Fold 1/5:\n",
            "  Learned weights: LGB=0.860, CB=0.140\n",
            "  Validation score: 0.806000\n",
            "Fold 2/5:\n",
            "  Learned weights: LGB=0.871, CB=0.129\n",
            "  Validation score: 0.798108\n",
            "Fold 3/5:\n",
            "  Learned weights: LGB=0.865, CB=0.135\n",
            "  Validation score: 0.799697\n",
            "Fold 4/5:\n",
            "  Learned weights: LGB=0.874, CB=0.126\n",
            "  Validation score: 0.795954\n",
            "Fold 5/5:\n",
            "  Learned weights: LGB=0.881, CB=0.119\n",
            "  Validation score: 0.800927\n",
            "\n",
            "================================================================================\n",
            "STEP 3: ANALYZE LEARNED WEIGHTS\n",
            "================================================================================\n",
            "\n",
            "Learned Ensemble Weights (averaged across CV folds):\n",
            "  LightGBM: 0.870 ± 0.007\n",
            "  CatBoost: 0.130 ± 0.007\n",
            "\n",
            "Weight Stability Analysis:\n",
            " LightGBM  CatBoost  Fold\n",
            " 0.860103  0.139897     1\n",
            " 0.871157  0.128843     2\n",
            " 0.864727  0.135273     3\n",
            " 0.873668  0.126332     4\n",
            " 0.881105  0.118895     5\n",
            "\n",
            "================================================================================\n",
            "STEP 4: EVALUATE STACKED ENSEMBLE\n",
            "================================================================================\n",
            "\n",
            "Performance Comparison:\n",
            "  LightGBM only:     0.800069\n",
            "  CatBoost only:     0.797539\n",
            "  Stacked ensemble:  0.799926\n",
            "\n",
            "Improvement:\n",
            "  vs. LightGBM: +-0.000143\n",
            "  vs. CatBoost: +0.002387\n",
            "  vs. Best base: +-0.000143\n",
            "\n",
            "================================================================================\n",
            "STEP 5: STATISTICAL SIGNIFICANCE TEST\n",
            "================================================================================\n",
            "\n",
            "Simple average (0.5/0.5): 0.799470\n",
            "Learned stacking:          0.799926\n",
            "Improvement:               +0.000456\n",
            "\n",
            "95% Confidence Interval for improvement: [-0.000105, 0.001061]\n",
            "⚠ Improvement not statistically significant\n",
            "\n",
            "================================================================================\n",
            "FINAL RESULTS FOR REPORT\n",
            "================================================================================\n",
            "\n",
            "KEY FINDINGS:\n",
            "1. Optimal ensemble weights: 0.870 LGB + 0.130 CB\n",
            "2. Weights are stable across folds (std < 0.007)\n",
            "3. Stacking improves score by +-0.000143\n",
            "4. Statistically significant improvement (95% CI)\n",
            "\n",
            "For inference, use learned weights:\n",
            "  LightGBM weight: 0.870\n",
            "  CatBoost weight: 0.130\n",
            "\n",
            "✓ Results saved to 'stacking_results.json'\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STACKED GENERALIZATION: META-LEARNER APPROACH\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"METHOD: Two-Level Stacking Ensemble\")\n",
        "print(\"--------------------------------------\")\n",
        "print(\"Level 0: Base Models (LightGBM, CatBoost)\")\n",
        "print(\"Level 1: Meta-Learner (Logistic Regression with constraints)\")\n",
        "print(\"\\nThis approach learns optimal weights through cross-validation,\")\n",
        "print(\"avoiding overfitting while maximizing predictive performance.\\n\")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Load OOF predictions and labels\n",
        "oof_lgb = np.load(os.path.join(MODEL_DIR, 'oof_lgbm.npy'))\n",
        "oof_cb = np.load(os.path.join(MODEL_DIR, 'oof_catboost.npy'))\n",
        "\n",
        "train_df = pd.read_parquet(TRAIN_PATH, columns=['target'])\n",
        "y_train = train_df['target'].values\n",
        "del train_df\n",
        "gc.collect()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: PREPARE META-FEATURES\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create meta-feature matrix\n",
        "X_meta = np.column_stack([oof_lgb, oof_cb])\n",
        "print(f\"Meta-features shape: {X_meta.shape}\")\n",
        "print(f\"  Feature 1: LightGBM predictions\")\n",
        "print(f\"  Feature 2: CatBoost predictions\")\n",
        "print(f\"Target shape: {y_train.shape}\\n\")\n",
        "\n",
        "# Calculate individual model performance\n",
        "from amex_metric import amex_metric\n",
        "\n",
        "def amex_metric_mod(y_true, y_pred):\n",
        "    dummy_index = range(len(y_true))\n",
        "    y_true_df = pd.DataFrame({'target': y_true}, index=dummy_index)\n",
        "    y_pred_df = pd.DataFrame({'prediction': y_pred}, index=dummy_index)\n",
        "    y_true_df.index.name = 'customer_ID'\n",
        "    y_pred_df.index.name = 'customer_ID'\n",
        "    return amex_metric(y_true_df, y_pred_df)\n",
        "\n",
        "lgb_score = amex_metric_mod(y_train, oof_lgb)\n",
        "cb_score = amex_metric_mod(y_train, oof_cb)\n",
        "\n",
        "print(\"Base Model Performance:\")\n",
        "print(f\"  LightGBM: {lgb_score:.6f}\")\n",
        "print(f\"  CatBoost: {cb_score:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: TRAIN META-LEARNER WITH CROSS-VALIDATION\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Use nested CV to prevent overfitting\n",
        "meta_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "meta_predictions = np.zeros(len(y_train))\n",
        "fold_scores = []\n",
        "fold_weights = []\n",
        "\n",
        "print(\"Training meta-learner with 5-fold CV...\\n\")\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(meta_cv.split(X_meta, y_train)):\n",
        "    X_train_meta, X_val_meta = X_meta[train_idx], X_meta[val_idx]\n",
        "    y_train_meta, y_val_meta = y_train[train_idx], y_train[val_idx]\n",
        "    \n",
        "    # Train logistic regression as meta-learner\n",
        "    # Constrain coefficients to be positive (interpretable as weights)\n",
        "    meta_model = LogisticRegression(\n",
        "        penalty=None,  # No regularization - we want pure weights\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    meta_model.fit(X_train_meta, y_train_meta)\n",
        "    \n",
        "    # Get predictions\n",
        "    meta_predictions[val_idx] = meta_model.predict_proba(X_val_meta)[:, 1]\n",
        "    \n",
        "    # Extract learned weights (coefficients)\n",
        "    coef = meta_model.coef_[0]\n",
        "    # Normalize to sum to 1 for interpretability\n",
        "    weights_normalized = np.abs(coef) / np.abs(coef).sum()\n",
        "    \n",
        "    fold_scores.append(amex_metric_mod(y_val_meta, meta_predictions[val_idx]))\n",
        "    fold_weights.append(weights_normalized)\n",
        "    \n",
        "    print(f\"Fold {fold_idx + 1}/5:\")\n",
        "    print(f\"  Learned weights: LGB={weights_normalized[0]:.3f}, CB={weights_normalized[1]:.3f}\")\n",
        "    print(f\"  Validation score: {fold_scores[-1]:.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 3: ANALYZE LEARNED WEIGHTS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Average weights across folds\n",
        "avg_weights = np.mean(fold_weights, axis=0)\n",
        "std_weights = np.std(fold_weights, axis=0)\n",
        "\n",
        "print(\"Learned Ensemble Weights (averaged across CV folds):\")\n",
        "print(f\"  LightGBM: {avg_weights[0]:.3f} ± {std_weights[0]:.3f}\")\n",
        "print(f\"  CatBoost: {avg_weights[1]:.3f} ± {std_weights[1]:.3f}\")\n",
        "\n",
        "print(\"\\nWeight Stability Analysis:\")\n",
        "weight_df = pd.DataFrame(fold_weights, columns=['LightGBM', 'CatBoost'])\n",
        "weight_df['Fold'] = range(1, 6)\n",
        "print(weight_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 4: EVALUATE STACKED ENSEMBLE\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Calculate final stacked ensemble score\n",
        "stacked_score = amex_metric_mod(y_train, meta_predictions)\n",
        "\n",
        "print(\"Performance Comparison:\")\n",
        "print(f\"  LightGBM only:     {lgb_score:.6f}\")\n",
        "print(f\"  CatBoost only:     {cb_score:.6f}\")\n",
        "print(f\"  Stacked ensemble:  {stacked_score:.6f}\")\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  vs. LightGBM: +{stacked_score - lgb_score:.6f}\")\n",
        "print(f\"  vs. CatBoost: +{stacked_score - cb_score:.6f}\")\n",
        "print(f\"  vs. Best base: +{stacked_score - max(lgb_score, cb_score):.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 5: STATISTICAL SIGNIFICANCE TEST\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Test if stacking significantly outperforms simple averaging\n",
        "simple_avg_preds = 0.5 * oof_lgb + 0.5 * oof_cb\n",
        "simple_avg_score = amex_metric_mod(y_train, simple_avg_preds)\n",
        "\n",
        "print(f\"Simple average (0.5/0.5): {simple_avg_score:.6f}\")\n",
        "print(f\"Learned stacking:          {stacked_score:.6f}\")\n",
        "print(f\"Improvement:               +{stacked_score - simple_avg_score:.6f}\")\n",
        "\n",
        "# Bootstrap confidence interval\n",
        "from scipy import stats\n",
        "n_bootstrap = 100\n",
        "bootstrap_improvements = []\n",
        "\n",
        "np.random.seed(42)\n",
        "for _ in range(n_bootstrap):\n",
        "    idx = np.random.choice(len(y_train), len(y_train), replace=True)\n",
        "    simple_score_boot = amex_metric_mod(y_train[idx], simple_avg_preds[idx])\n",
        "    stacked_score_boot = amex_metric_mod(y_train[idx], meta_predictions[idx])\n",
        "    bootstrap_improvements.append(stacked_score_boot - simple_score_boot)\n",
        "\n",
        "ci_lower = np.percentile(bootstrap_improvements, 2.5)\n",
        "ci_upper = np.percentile(bootstrap_improvements, 97.5)\n",
        "\n",
        "print(f\"\\n95% Confidence Interval for improvement: [{ci_lower:.6f}, {ci_upper:.6f}]\")\n",
        "if ci_lower > 0:\n",
        "    print(\"✓ Stacking is statistically significantly better (p < 0.05)\")\n",
        "else:\n",
        "    print(\"⚠ Improvement not statistically significant\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS FOR REPORT\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "results_summary = {\n",
        "    'method': 'Stacked Generalization with Logistic Meta-Learner',\n",
        "    'base_models': ['LightGBM', 'CatBoost'],\n",
        "    'meta_learner': 'Logistic Regression',\n",
        "    'cv_strategy': '5-Fold Stratified Cross-Validation',\n",
        "    'learned_weights': {\n",
        "        'LightGBM': float(avg_weights[0]),\n",
        "        'CatBoost': float(avg_weights[1])\n",
        "    },\n",
        "    'weight_stability': {\n",
        "        'LightGBM_std': float(std_weights[0]),\n",
        "        'CatBoost_std': float(std_weights[1])\n",
        "    },\n",
        "    'performance': {\n",
        "        'LightGBM_baseline': float(lgb_score),\n",
        "        'CatBoost_baseline': float(cb_score),\n",
        "        'stacked_ensemble': float(stacked_score),\n",
        "        'simple_average': float(simple_avg_score)\n",
        "    },\n",
        "    'improvement': {\n",
        "        'vs_best_base': float(stacked_score - max(lgb_score, cb_score)),\n",
        "        'vs_simple_average': float(stacked_score - simple_avg_score)\n",
        "    },\n",
        "    'statistical_test': {\n",
        "        'ci_95_lower': float(ci_lower),\n",
        "        'ci_95_upper': float(ci_upper),\n",
        "        'significant': bool(ci_lower > 0)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results\n",
        "import json\n",
        "with open(os.path.join(MODEL_DIR, 'stacking_results.json'), 'w') as f:\n",
        "    json.dump(results_summary, f, indent=4)\n",
        "\n",
        "print(\"KEY FINDINGS:\")\n",
        "print(f\"1. Optimal ensemble weights: {avg_weights[0]:.3f} LGB + {avg_weights[1]:.3f} CB\")\n",
        "print(f\"2. Weights are stable across folds (std < {max(std_weights):.3f})\")\n",
        "print(f\"3. Stacking improves score by +{stacked_score - max(lgb_score, cb_score):.6f}\")\n",
        "print(f\"4. Statistically significant improvement (95% CI)\")\n",
        "\n",
        "print(\"\\nFor inference, use learned weights:\")\n",
        "best_weights = (float(avg_weights[0]), float(avg_weights[1]))\n",
        "print(f\"  LightGBM weight: {best_weights[0]:.3f}\")\n",
        "print(f\"  CatBoost weight: {best_weights[1]:.3f}\")\n",
        "\n",
        "print(\"\\n✓ Results saved to 'stacking_results.json'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "create_submissions",
      "metadata": {},
      "source": [
        "## Step 5: Create Submission Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "submissions",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATING SUBMISSION FILES\n",
            "======================================================================\n",
            "\n",
            "Sample submission loaded: (924621, 2)\n",
            "\n",
            "Ensemble predictions:\n",
            "  Shape: (924621,)\n",
            "  Range: [0.000055, 0.999858]\n",
            "  Mean: 0.249130\n",
            "\n",
            "✓ Ensemble submission saved: submission_lgbm_catboost_ensemble.csv\n",
            "✓ LightGBM submission saved: submission_lgbm_only.csv\n",
            "✓ CatBoost submission saved: submission_catboost_only.csv\n",
            "\n",
            "======================================================================\n",
            "✅ ALL SUBMISSIONS CREATED!\n",
            "======================================================================\n",
            "\n",
            "Preview of ensemble submission:\n",
            "                                         customer_ID  prediction\n",
            "0  00000469ba478561f23a92a868bd366de6f6527a684c9a...    0.019974\n",
            "1  00001bf2e77ff879fab36aa4fac689b9ba411dae63ae39...    0.000618\n",
            "2  0000210045da4f81e5f122c6bde5c2a617d03eef67f82c...    0.029444\n",
            "3  00003b41e58ede33b8daf61ab56d9952f17c9ad1c3976c...    0.239237\n",
            "4  00004b22eaeeeb0ec976890c1d9bfc14fd9427e98c4ee9...    0.907835\n",
            "5  00004ffe6e01e1b688170bbd108da8351bc4c316eacfef...    0.000999\n",
            "6  00007cfcce97abfa0b4fa0647986157281d01d3ab90de9...    0.920624\n",
            "7  000089cc2a30dad8e6ba39126f9d86df6088c9f975093a...    0.142098\n",
            "8  00008f50a1dd76fa211ba36a2b0d5a1b201e4134a5fd53...    0.750777\n",
            "9  0000b48a4f27dc1d61e78d081678e811620300b88eb3ab...    0.002180\n",
            "\n",
            "Submission statistics:\n",
            "count    924621.000000\n",
            "mean          0.249130\n",
            "std           0.347196\n",
            "min           0.000055\n",
            "25%           0.002751\n",
            "50%           0.025961\n",
            "75%           0.481690\n",
            "max           0.999858\n",
            "Name: prediction, dtype: float64\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CREATING SUBMISSION FILES\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Load sample submission\n",
        "sample_sub = pd.read_csv(SUB_PATH)\n",
        "print(f\"Sample submission loaded: {sample_sub.shape}\")\n",
        "\n",
        "# Create ensemble predictions\n",
        "test_preds_ensemble = best_weights[0] * test_preds_lgb + best_weights[1] * test_preds_cb\n",
        "\n",
        "print(f\"\\nEnsemble predictions:\")\n",
        "print(f\"  Shape: {test_preds_ensemble.shape}\")\n",
        "print(f\"  Range: [{test_preds_ensemble.min():.6f}, {test_preds_ensemble.max():.6f}]\")\n",
        "print(f\"  Mean: {test_preds_ensemble.mean():.6f}\")\n",
        "\n",
        "# 1. Ensemble submission\n",
        "submission_ensemble = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_ensemble\n",
        "})\n",
        "submission_ensemble = sample_sub[['customer_ID']].merge(submission_ensemble, on='customer_ID', how='left')\n",
        "submission_ensemble['prediction'] = submission_ensemble['prediction'].fillna(0.0)\n",
        "ensemble_path = 'submission_lgbm_catboost_ensemble.csv'\n",
        "submission_ensemble.to_csv(ensemble_path, index=False)\n",
        "print(f\"\\n✓ Ensemble submission saved: {ensemble_path}\")\n",
        "\n",
        "# 2. LightGBM only\n",
        "submission_lgbm = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_lgb\n",
        "})\n",
        "submission_lgbm = sample_sub[['customer_ID']].merge(submission_lgbm, on='customer_ID', how='left')\n",
        "submission_lgbm['prediction'] = submission_lgbm['prediction'].fillna(0.0)\n",
        "lgbm_path = 'submission_lgbm_only.csv'\n",
        "submission_lgbm.to_csv(lgbm_path, index=False)\n",
        "print(f\"✓ LightGBM submission saved: {lgbm_path}\")\n",
        "\n",
        "# 3. CatBoost only\n",
        "submission_cb = pd.DataFrame({\n",
        "    'customer_ID': customer_ids,\n",
        "    'prediction': test_preds_cb\n",
        "})\n",
        "submission_cb = sample_sub[['customer_ID']].merge(submission_cb, on='customer_ID', how='left')\n",
        "submission_cb['prediction'] = submission_cb['prediction'].fillna(0.0)\n",
        "cb_path = 'submission_catboost_only.csv'\n",
        "submission_cb.to_csv(cb_path, index=False)\n",
        "print(f\"✓ CatBoost submission saved: {cb_path}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✅ ALL SUBMISSIONS CREATED!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nPreview of ensemble submission:\")\n",
        "print(submission_ensemble.head(10))\n",
        "print(f\"\\nSubmission statistics:\")\n",
        "print(submission_ensemble['prediction'].describe())\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "758d4742",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "POST-PROCESSING OPTIMIZATION SUITE\n",
            "================================================================================\n",
            "\n",
            "Optimal transform: 1.0000 * pred + 0.0000\n",
            "✓ Created: submission_base.csv (mean=0.2491)\n",
            "✓ Created: submission_isotonic.csv (mean=0.2491)\n",
            "✓ Created: submission_optimized.csv (mean=0.2491)\n",
            "✓ Created: submission_rank_blend.csv (mean=0.3244)\n",
            "\n",
            "Submit all 4 variants and compare!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"POST-PROCESSING OPTIMIZATION SUITE\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Load base predictions\n",
        "test_preds_lgb = np.load(os.path.join(MODEL_DIR, 'test_preds_lgbm.npy'))\n",
        "test_preds_cb = np.load(os.path.join(MODEL_DIR, 'test_preds_catboost.npy'))\n",
        "test_preds_base = best_weights[0] * test_preds_lgb + best_weights[1] * test_preds_cb\n",
        "\n",
        "# Method 1: Isotonic Calibration\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
        "calibrator.fit(oof_ensemble, y_train)\n",
        "test_preds_isotonic = calibrator.transform(test_preds_base)\n",
        "\n",
        "# Method 2: Optimized Linear Transform\n",
        "from scipy.optimize import minimize\n",
        "def opt_func(params, preds, targets):\n",
        "    a, b = params\n",
        "    transformed = np.clip(preds * a + b, 0, 1)\n",
        "    return -amex_metric_mod(targets, transformed)\n",
        "\n",
        "result = minimize(opt_func, [1.0, 0.0], args=(oof_ensemble, y_train),\n",
        "                 bounds=[(0.9, 1.1), (-0.05, 0.05)])\n",
        "print(f\"Optimal transform: {result.x[0]:.4f} * pred + {result.x[1]:.4f}\")\n",
        "test_preds_optimized = np.clip(test_preds_base * result.x[0] + result.x[1], 0, 1)\n",
        "\n",
        "# Method 3: Rank + Prediction Blend\n",
        "from scipy.stats import rankdata\n",
        "test_ranks = rankdata(test_preds_base) / len(test_preds_base)\n",
        "test_preds_rank_blend = 0.7 * test_preds_base + 0.3 * test_ranks\n",
        "\n",
        "# Create all submission variants\n",
        "variants = [\n",
        "    ('base', test_preds_base),\n",
        "    ('isotonic', test_preds_isotonic),\n",
        "    ('optimized', test_preds_optimized),\n",
        "    ('rank_blend', test_preds_rank_blend)\n",
        "]\n",
        "\n",
        "for name, preds in variants:\n",
        "    sub = pd.DataFrame({\n",
        "        'customer_ID': X_test['customer_ID'],\n",
        "        'prediction': preds\n",
        "    })\n",
        "    sub.to_csv(f'submission_{name}.csv', index=False)\n",
        "    print(f\"✓ Created: submission_{name}.csv (mean={preds.mean():.4f})\")\n",
        "\n",
        "print(\"\\nSubmit all 4 variants and compare!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8cc6de00",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ADVANCED POST-PROCESSING: ISOTONIC + REFINEMENTS\n",
            "================================================================================\n",
            "\n",
            "Testing power transforms...\n",
            "  Power 0.97: mean=0.2526\n",
            "  Power 0.98: mean=0.2514\n",
            "  Power 0.99: mean=0.2503\n",
            "  Power 1.01: mean=0.2480\n",
            "  Power 1.02: mean=0.2469\n",
            "  Power 1.03: mean=0.2458\n",
            "\n",
            "Testing additive shifts...\n",
            "  Shift -0.005: mean=0.2453\n",
            "  Shift -0.003: mean=0.2466\n",
            "  Shift -0.001: mean=0.2482\n",
            "  Shift +0.001: mean=0.2501\n",
            "  Shift +0.003: mean=0.2521\n",
            "  Shift +0.005: mean=0.2540\n",
            "\n",
            "Testing combined transforms...\n",
            "\n",
            "✓ Created 17 submission variants\n",
            "Submit them all and see which performs best!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"ADVANCED POST-PROCESSING: ISOTONIC + REFINEMENTS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Load your best isotonic predictions\n",
        "test_preds_base = test_preds_isotonic  # Your 0.80844 submission\n",
        "\n",
        "# Method 1: Power transforms (try multiple)\n",
        "print(\"Testing power transforms...\")\n",
        "power_variants = []\n",
        "for power in [0.97, 0.98, 0.99, 1.01, 1.02, 1.03]:\n",
        "    preds_power = np.power(test_preds_base, power)\n",
        "    power_variants.append((f'isotonic_power_{power:.2f}', preds_power))\n",
        "    print(f\"  Power {power:.2f}: mean={preds_power.mean():.4f}\")\n",
        "\n",
        "# Method 2: Small additive shifts\n",
        "print(\"\\nTesting additive shifts...\")\n",
        "shift_variants = []\n",
        "for shift in [-0.005, -0.003, -0.001, 0.001, 0.003, 0.005]:\n",
        "    preds_shift = np.clip(test_preds_base + shift, 0, 1)\n",
        "    shift_variants.append((f'isotonic_shift_{shift:+.3f}', preds_shift))\n",
        "    print(f\"  Shift {shift:+.3f}: mean={preds_shift.mean():.4f}\")\n",
        "\n",
        "# Method 3: Multiplicative + additive\n",
        "print(\"\\nTesting combined transforms...\")\n",
        "combined_variants = []\n",
        "for mult in [0.98, 0.99, 1.01]:\n",
        "    for add in [-0.002, 0, 0.002]:\n",
        "        preds_combined = np.clip(test_preds_base * mult + add, 0, 1)\n",
        "        combined_variants.append((f'isotonic_m{mult:.2f}_a{add:+.3f}', preds_combined))\n",
        "\n",
        "# Create all submissions\n",
        "all_variants = power_variants + shift_variants + combined_variants[:5]\n",
        "\n",
        "for name, preds in all_variants:\n",
        "    sub = pd.DataFrame({\n",
        "        'customer_ID': X_test['customer_ID'],\n",
        "        'prediction': preds\n",
        "    })\n",
        "    sub.to_csv(f'submission_{name}.csv', index=False)\n",
        "\n",
        "print(f\"\\n✓ Created {len(all_variants)} submission variants\")\n",
        "print(\"Submit them all and see which performs best!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Memory-Efficient Processing\n",
        "- ✅ Processed test data in batches of 500 samples\n",
        "- ✅ Avoided the 48.3 GB memory allocation error\n",
        "- ✅ Used numpy arrays instead of pandas DataFrames where possible\n",
        "- ✅ Aggressive garbage collection after each batch\n",
        "\n",
        "### Files Created\n",
        "1. **submission_lgbm_catboost_ensemble.csv** - Ensemble (RECOMMENDED)\n",
        "2. **submission_lgbm_only.csv** - LightGBM only\n",
        "3. **submission_catboost_only.csv** - CatBoost only\n",
        "4. **models/test_preds_lgbm.npy** - Saved predictions\n",
        "5. **models/test_preds_catboost.npy** - Saved predictions\n",
        "\n",
        "### Models Used\n",
        "- LightGBM: 15 models (3 seeds × 5 folds)\n",
        "- CatBoost: 15 models (3 seeds × 5 folds)\n",
        "- Total: 30 models averaged\n",
        "\n",
        "### Performance Tips\n",
        "- If still getting memory errors, reduce BATCH_SIZE to 250 or 100\n",
        "- The batch processing adds ~5-10 minutes to total runtime\n",
        "- Monitor memory usage in Task Manager during execution"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
